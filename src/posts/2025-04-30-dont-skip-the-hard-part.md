---
layout: post
title: >
  AI Adoption: The Cost of Skipping the Hard Part
---

<!-- summary -->

**Competence in the Age of AI**

Leadership in legacy organisations are increasingly impose AI to speed up delivery in the short term. Almost in a bid to forget what it means to 'go quickly' in the context of a large codebase.

While the sentiment is to be expected, as developers, we need ensure we do not loose sight of what makes us valuable; our hard-won skills. In good design and intentionality in how we scale our systems, and retain our human agency over them.

<!-- /summary -->

Adoption of LLMs in software teams, especially at non-tech-native firms has been tentative; even suspect. Some [mandating](https://leaddev.com/culture/ai-coding-mandates-are-driving-developers-to-the-brink) and even stack-ranking its people on adoption into workflows, in a panicked attempt to keep up and leverage any ‘productivity’ gain possible _for the business._ As I wrote previously, a more meaningful, intentional value of AI exists beyond quicker output of the wrong thing. What is it to be _productive_ if teams are still _ineffective?_

Indeed, the one thing we know about the thing we _are_ building, is that _it is wrong._ Our goal as builders of useful software, is to find out _where_, and how we can make it _less wrong_ as soon and as often as possible. All the agile ceremonies, words and artefacts are mere performance, if they do not _assist_ us in that endeavour.

In agile, the only measure of progress, is _working software_.

Meanwhile, as Product Engineers, many rightly view AI contribution to the codebase with skepticism.

My experience has been that the _fitness_ of AI-generated code in the first place diminishes the less proximal the problem space is. That is to say, AI is not well suited to the long term maintainability.

Given that the [cost of software is roughly equivalent to the cost of change](https://www.youtube.com/watch?v=ZHpQs46xizQ) (Kent Beck), we must prioritise _optionality_ in our software design; something AI is presently and in my very experience _bad at._ Unless we _know_ from experience how to guard against it, we risk enabling AI to compromise our codebases and Products with spiralling costs of change, and a diminishing user experience we'll not appreciate until it's too late.

**Getting from 0-60**

That said, AI can, and _does_ alleviate much toil in rote code generation; upgrading libraries, prototypes, releases, suggesting bug sources, maybe even writing tests.

It gets us from ‘_0-60’_ much quicker, allowing a technical Product team to quickly _pilot a hypothesis_ and get real, early feedback; data with which to decide the bets worth pursing.

Because that’s what we’re doing with Products: making a bet; a hypothesis and testing it often, and AI allows us to tighten that cycle. If we do not, competitors will. The baseline of being able to quickly learn and iterate has increased for all of us.

Unsurprising then that AI is being increasingly mandated, albeit with bounds and tools of their choosing, but businesses are obligated to respond to enable this degree of rapid iteration. But this also means the _manner in which_ they chose adopt AI in support of an existing competitive advantage is existentially important to ensure we're optimising for the right outcomes that the business cares about.

Requiring developers to adopt specific AI tools in the name of expediency, feels an out-of-touch and short-sighted move, and undermines the liberty of ones’ own professional judgement in adopting the tools that best help our workflow, sharing what we find with _each other_ toward our common goals.

I'd counsel leader navigating this new tech landscape to be mindful of the potential for exposing teams to second-order effects; not just in skill atrophy and our own existential risk, but the myriad ways in which AI introduces wrong solutions, arbitrary tech choices and coupling into the code, as recounted [in this article](https://martinfowler.com/articles/exploring-gen-ai/13-role-of-developer-skills.html) from Martin Fowlers’ blog; evaluating an LLM’s impact on dev skills.

**Devs against entropy**

As developers in this changing technical and political corporate landscape, we must guard against our own hard-learned skills becoming undermined. That is, knowing what changeable, decoupled software design looks like and stepping=up to increased _advocacy_ and education of the wider business, of its importance for continued success of our products, and why it's necessary to defend.

A team losing its grasp on changeability is a team losing agency. On a long enough timeline, that’s how a product dies.

**Its fun to be competent**

I don't think the gains we get as developers will be all that transformational when applied to maintaining an existing, mature codebase. We still need to learn the things, and solve problems ourselves. Now just with the overhead of untangling and second-guessing AI's code if we choose to use it. We don't get off that easy.

While humans tendency seeks the easy route, there are no shortcuts to becoming _competent_ at software design. This will become even more, not less, crucial for working effectively _with_ AI as studious _Lead devs_ to the AI, dutifully reviewing its output.

Until the organisation is fully weaned off humans all-together.

Therefore, as engineers we have responsibility _to ourselves_ to not become dependant. Else we are _choosing_ learned helplessness, not ‘_being efficient.’_

Real productivity starts with intentionality; consider what we _want_ a solution to look like first before using the assistant to create it.

**The call of autocomplete**

Are we _in this moment_ making a conscious decision to forego learning? Will we or our colleagues need to pay that up or make sense of this? If so, are we happy with that cost?

It’s been personally rattling to notice this creeping in; reflexively _expecting_ a solution to (often incorrectly) come out, and having to be very diligent to notice this temptation.

We may need to hold nerve for just a moment, against the background noise of urgency.

We must remember in that choice point, organisations’ goals are impersonal: to maximise productivity and efficiency by any means necessary.
This is fine; it’s in their DNA and is their responsibility.

But we as creators have a choice. Responsibility to ourselves to remain effective long-term, and so be dilligent in maintaining our technical fitness and critical thinking.

**It is fun to be competent, and there are no shortcuts.**

Productivity matters, but only when have _earned_ the ability through our (mis)adventures in software development, _maintenance,_ and deep understanding of good design. We can _decide_ when to integrate AI into our workflows in ways that enhance our abilities, not override our judgment.
